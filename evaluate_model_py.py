# -*- coding: utf-8 -*-
"""evaluate_model.py

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1--jXRn4mKjSZpZcDy7GyVASxUUDJ4eZ6
"""

from sklearn.metrics import classification_report, confusion_matrix

prediction = model.predict(x_test)

classification_report(y_test, prediction)

binary_prediction = (prediction > 0.5).astype(int)

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_auc_score, roc_curve

# Precision
precision = precision_score(y_test, binary_prediction)
print(f'Precision: {precision}')

# Recall
recall = recall_score(y_test, binary_prediction)
print(f'Recall: {recall}')

# F1 Score
f1 = f1_score(y_test, binary_prediction)
print(f'F1 Score: {f1}')

# ROC-AUC
roc_auc = roc_auc_score(y_test, binary_prediction)
print(f'ROC-AUC: {roc_auc}')

# ROC Curve
fpr, tpr, thresholds = roc_curve(y_test, binary_prediction)

# Accuracy
accuracy = accuracy_score(y_test, binary_prediction)
print(f'Accuracy: {accuracy}')

# Confusion Matrix
cm = confusion_matrix(y_test, binary_prediction)
print(f'Confusion Matrix:\n{cm}')

import matplotlib.pyplot as plt

loss = pd.DataFrame(model1.history.history)
loss.plot()